{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d4f4d-eb4e-4665-aa36-4e081154bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f023c-104a-438f-9093-620d6fe54882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1696b-720e-4be4-b700-a1d98cdec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "#print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69b196-5445-4a4e-aede-a361966bfe4f",
   "metadata": {},
   "source": [
    "## load mitre sigma master csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48186f9e-8306-4dac-a8ac-424f6761f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "mitre_sigma = pd.read_csv(\"MITRE/mitreembed_master_Chroma.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991295ad-7071-446c-a9f1-a966aea93e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "# download embeddings model\n",
    "original_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "# reload model using langchain wrapper\n",
    "original_model.save('./')\n",
    "\n",
    "embedding_model_path = './'\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fa4bb-d6ec-42b1-af40-1a625b5f18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129a5d6-c08f-4b70-8701-32c503cad33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c3500-e065-49df-befb-dc6a9b6ee3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set chromadb \n",
    "#import pydantic\n",
    "#from pydantic_settings import BaseSettings\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "# define logic for embeddings storage\n",
    "chromadb_path = './'\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_client.get_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d15d09-9df0-4b45-be28-f615661d34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    " \n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import re\n",
    "#import mlflow\n",
    " \n",
    "import pandas as pd\n",
    "# assemble product documents in required format (id, text)\n",
    "documents = (\n",
    "  DataFrameLoader(\n",
    "    #CISA_df_pd,\n",
    "    mitre_sigma,\n",
    "    page_content_column='Body'\n",
    "    )\n",
    "    .load()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a1cd2-4807-4e55-9f2b-4b9996f0fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logic for embeddings storage\n",
    "chromadb_path = './'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "  documents=documents, \n",
    "  embedding=embedding_model, \n",
    "  persist_directory=chromadb_path, \n",
    "  #collection_name = 'CISA_MITRE'\n",
    "  )\n",
    " \n",
    "# persist vector db to storage\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb1449-ffae-45c3-8ef7-131826f4a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete vectors\n",
    "#vectordb.delete_collection()\n",
    "\n",
    "#count documents \n",
    "vectordb._collection.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218ef8f-5498-4cad-86d2-1318325301d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine a vector db record\n",
    "rec= vectordb._collection.peek(1)\n",
    "print('Metadatas:  ', rec['metadatas'])\n",
    "print('Documents:  ', rec['documents'])\n",
    "print('ids:        ', rec['ids']) \n",
    "print('embeddings: ', rec['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1b660-6c10-41ae-ae57-b0a84d99401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search_with_score(\"8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7228c-49e5-426f-8218-d3d0088d52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search_with_relevance_scores(\"MoveIt Vulnerability\", k=50, score_threshold=0.80, search_type=\"hybrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7547307-f772-4bc1-b9f1-ecdaa9854f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductSearchWrapper:\n",
    "    def __init__(self, embedding_model, chromadb, max_results=25):\n",
    "        # Retrieve embedding model\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_path)\n",
    "\n",
    "        # Retrieve vectordb contents\n",
    "        self._vectordb = Chroma(persist_directory=chromadb_path, embedding_function=self.embedding_model)\n",
    "\n",
    "        # Set number of results to returnd\n",
    "        self._max_results = max_results\n",
    "\n",
    "    def predict(self, query):\n",
    "        # Perform search on embeddings\n",
    "        raw_results = self._vectordb.similarity_search_with_score(query, k=self._max_results)\n",
    "\n",
    " \n",
    "        # get lists of of scores, descriptions and ids from raw results\n",
    "        scores, Body, Subject, Date, filepath, Source = zip(\n",
    "          *[(r[1], r[0].page_content, r[0].metadata['Subject'], r[0].metadata['Date'], r[0].metadata['filepath'], \n",
    "             r[0].metadata['Source']) \n",
    "             for r in raw_results] \n",
    "          )\n",
    "        results_pd = pd.DataFrame({        \n",
    "            #'Body':Body,\n",
    "            'Subject':Subject,\n",
    "            'Date':Date,\n",
    "            'Body':Body,\n",
    "            'filepath':filepath,\n",
    "            'Source':Source,\n",
    "            'score':scores\n",
    "          }).sort_values(axis=0, by='score', ascending=True)\n",
    "\n",
    "        #set return value\n",
    "        return results_pd\n",
    "        #return raw_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aceb55-aea3-44e6-b6a5-956e75c85fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "#model objective:\n",
    "\"\"\"\n",
    "The model is intented to be used as a sentence and short paragraph encoder. \n",
    "Given an input text, it ouptuts a vector which captures the semantic information. \n",
    "The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n",
    "\"\"\"\n",
    "\n",
    "model = ProductSearchWrapper(embedding_model=embedding_model_path, chromadb=chromadb_path, max_results=25)\n",
    "\n",
    "\n",
    "# Call the model's predict method with a query\n",
    "#query = \"Which is the most recent CISA advisory on Ivanti\"\n",
    "query = \"MoveIT vulnerability\"\n",
    "results = model.predict(query)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "#df_results\n",
    "\n",
    "#df_results.to_csv('result_test.csv')\n",
    "# response_text = df_results[['Subject', 'Date','filepath','score']]\n",
    "# response_text\n",
    "#str(response_text.iloc[0]['Subject'])\n",
    "\n",
    "#df_results.iloc[0]\n",
    "#df_results.iloc[0]['Body']\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cf75b-e75f-4f04-9609-06d1b18f23e0",
   "metadata": {},
   "source": [
    "## LLM integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97039b-5e86-4b66-82bb-fd91558f1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dcbbcb-2482-48ed-b6bd-d806349de5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cf59f-840d-45fa-a29b-a8507f0313d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize LLM wizardlm-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36cde3-7159-412d-bc63-671ff0126651",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 35  # Metal set to 1 is enough.\n",
    "#n_batch = 1042  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    #model_path=\"Lilly/Lily-7B-Instruct-v0.2.Q5_K_M.gguf\",\n",
    "    model_path=\"./wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.gguf.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    " #   n_batch=n_batch,\n",
    " #   temperature=0.75,\n",
    "    max_tokens=2000,\n",
    " #   top_p=1,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ae02b-944d-4c93-aad5-eca9a211bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "#!pip install tf-nightly\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bcf44-5af7-4407-8d29-4db753c75862",
   "metadata": {},
   "source": [
    "## intialize koboldcpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68d8b8-db3c-4f55-9d9f-0b3c844327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/p/9f890e6960f3\n",
    "# Successfully uninstalled langchain-0.0.198\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "vectordb = Chroma(persist_directory=chromadb_path, embedding_function=embedding_model)\n",
    "#retrievers\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm,\n",
    "#     retriever=vectordb.as_retriever()\n",
    "# )\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
    "\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    #chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")\n",
    "from langchain.llms import KoboldApiLLM\n",
    "llm = KoboldApiLLM(endpoint=\"http://localhost:5001\", max_length=80)\n",
    "#llm = KoboldApiLLM(endpoint=\"http://54.165.180.47:5001\", max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2a4f0-1435-4091-90fd-f30d56eeebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa(\"Tell me about the MoveIT vulnerability.\")\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
